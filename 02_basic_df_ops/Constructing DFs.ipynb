{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (producct id, product name, qty)\n",
    "data = [[1, \"product1\", 10],\n",
    "        [2, \"product2\", 20]]\n",
    "\n",
    "# Defining schema for dataframes\n",
    "# StructField(column_name, column_type, nullable?)\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField('product_id', IntegerType(), False),\n",
    "    StructField('product_name', StringType(), False),\n",
    "    StructField('qty', IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---+\n",
      "|product_id|product_name|qty|\n",
      "+----------+------------+---+\n",
      "|         1|    product1| 10|\n",
      "|         2|    product2| 20|\n",
      "+----------+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = false)\n",
      " |-- product_name: string (nullable = false)\n",
      " |-- qty: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated schema inferring\n",
    "df2 = spark.createDataFrame(data, ['product_id', 'product_name', 'qty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count\r\n",
      "United States,Romania,15\r\n",
      "United States,Croatia,1\r\n",
      "United States,Ireland,344\r\n",
      "Egypt,United States,15\r\n",
      "United States,India,62\r\n",
      "United States,Singapore,1\r\n",
      "United States,Grenada,62\r\n",
      "Costa Rica,United States,588\r\n",
      "Senegal,United States,40\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 spark_training_baseline/data/flights.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "csv_schema = StructType([\n",
    "    # StructField (name, dataType, nullable, metadata)\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"count\", LongType(), False)    \n",
    "])\n",
    "\n",
    "# spark.read is a DataFrameReader singleton class\n",
    "df = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option('header', 'true') \\\n",
    "    .schema(csv_schema) \\\n",
    "    .load('spark_training_baseline/data/flights.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('flights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM flights LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=1, orderBy=[total_outbound#312L DESC NULLS LAST], output=[ORIGIN_COUNTRY_NAME#45,total_outbound#312L])\n",
      "+- *(2) HashAggregate(keys=[ORIGIN_COUNTRY_NAME#45], functions=[sum(count#46L)])\n",
      "   +- Exchange hashpartitioning(ORIGIN_COUNTRY_NAME#45, 200), true, [id=#451]\n",
      "      +- *(1) HashAggregate(keys=[ORIGIN_COUNTRY_NAME#45], functions=[partial_sum(count#46L)])\n",
      "         +- FileScan csv [ORIGIN_COUNTRY_NAME#45,count#46L] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/andras/ipython_spark/spark_training_baseline/data/flights.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ORIGIN_COUNTRY_NAME:string,count:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Excercise: find the origin country which has the largest number outbound flights from\n",
    "df_spark = spark.sql(\"\"\"\n",
    "SELECT ORIGIN_COUNTRY_NAME, sum(count) as total_outbound\n",
    "FROM flights\n",
    "GROUP BY ORIGIN_COUNTRY_NAME\n",
    "ORDER BY total_outbound DESC\n",
    "LIMIT 1;\n",
    "\"\"\")\n",
    "df_spark.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=1, orderBy=[total_outbound#323L DESC NULLS LAST], output=[ORIGIN_COUNTRY_NAME#45,total_outbound#323L])\n",
      "+- *(2) HashAggregate(keys=[ORIGIN_COUNTRY_NAME#45], functions=[sum(count#46L)])\n",
      "   +- Exchange hashpartitioning(ORIGIN_COUNTRY_NAME#45, 200), true, [id=#475]\n",
      "      +- *(1) HashAggregate(keys=[ORIGIN_COUNTRY_NAME#45], functions=[partial_sum(count#46L)])\n",
      "         +- FileScan csv [ORIGIN_COUNTRY_NAME#45,count#46L] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/andras/ipython_spark/spark_training_baseline/data/flights.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ORIGIN_COUNTRY_NAME:string,count:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_python = df.groupBy('ORIGIN_COUNTRY_NAME').agg(sum('count').alias('total_outbound')) \\\n",
    "    .sort(desc('total_outbound')).limit(1)\n",
    "df_python.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|                col1|               col2| col3|\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|COUNT|\n",
      "|       UNITED STATES|            ROMANIA|   15|\n",
      "|       UNITED STATES|            CROATIA|    1|\n",
      "|       UNITED STATES|            IRELAND|  344|\n",
      "|               EGYPT|      UNITED STATES|   15|\n",
      "|       UNITED STATES|              INDIA|   62|\n",
      "|       UNITED STATES|          SINGAPORE|    1|\n",
      "|       UNITED STATES|            GRENADA|   62|\n",
      "|          COSTA RICA|      UNITED STATES|  588|\n",
      "|             SENEGAL|      UNITED STATES|   40|\n",
      "|             MOLDOVA|      UNITED STATES|    1|\n",
      "|       UNITED STATES|       SINT MAARTEN|  325|\n",
      "|       UNITED STATES|   MARSHALL ISLANDS|   39|\n",
      "|              GUYANA|      UNITED STATES|   64|\n",
      "|               MALTA|      UNITED STATES|    1|\n",
      "|            ANGUILLA|      UNITED STATES|   41|\n",
      "|             BOLIVIA|      UNITED STATES|   30|\n",
      "|       UNITED STATES|           PARAGUAY|    6|\n",
      "|             ALGERIA|      UNITED STATES|    4|\n",
      "|TURKS AND CAICOS ...|      UNITED STATES|  230|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. How to read a (unstructed) text file to RDD\n",
    "rdd = sc.textFile('spark_training_baseline/data/flights.csv')\n",
    "rdd2 = rdd.map(lambda x: x.upper()).map(lambda x: x.split(','))\n",
    "rdd2.take(10)\n",
    "\n",
    "# 2. How to convert RDD to dataframe\n",
    "df_from_rdd = rdd2.toDF(['col1', 'col2', 'col3'])\n",
    "df_from_rdd.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
