{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Dataframes operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema can be inferred automatedly. Spark also can infer schema from semi-structured data formats, e.g. JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load('../data/flights.json').schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we also can define schema explicitly. Automated schema discovery is not always robust, sometimes struggling to detect the right type of each columns, e.g. precision issues (int detected instead of long). Moreover, inferring schema sometimes can be a bit slolw.\n",
    "\n",
    "**Parameter of StructField:**\n",
    "\n",
    "name – string, name of the field.\n",
    "\n",
    "dataType – DataType of the field.\n",
    "\n",
    "nullable – boolean, whether the field can be null (None) or not.\n",
    "\n",
    "metadata – a dict from string to simple type that can be toInternald to JSON automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "manualSchema = StructType([\n",
    "    # StructField (name, dataType, nullable, metadata)\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"count\", LongType(), False)    \n",
    "])\n",
    "flightsDF = spark.read.format(\"json\").schema(manualSchema).load('../data/flights.json')\n",
    "flightsDF.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataframes\n",
    "\n",
    "There are several ways for creating dataframes. The most straightforward way is to create a dataframe from the content of file(s). However, it is also possible to create dataframes programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| colA|colB|\n",
      "+-----+----+\n",
      "|Hello|  12|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "mySchema = StructType([\n",
    "    StructField(\"colA\", StringType(), True),\n",
    "    StructField(\"colB\", LongType(), True)\n",
    "])\n",
    "\n",
    "# DFs basically are just collections of Rows. We can create a Row object in the following way:\n",
    "myRow = Row(\"Hello\", 12)\n",
    "myDF = spark.createDataFrame([myRow], mySchema)\n",
    "myDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a Dataframe from RDD, if our RDD elements are also Row objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  colA|colB|\n",
      "+------+----+\n",
      "|Hello!|  12|\n",
      "|Hallo!|  23|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collection = [Row(\"Hello!\", 12), (\"Hallo!\", 23)]\n",
    "rdd = spark.sparkContext.parallelize(collection)\n",
    "df = spark.createDataFrame(rdd, mySchema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection (selection of columns)\n",
    "\n",
    "Projection allows you to select specific columns only from the DF. This is very similar to SQL SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.select(\"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------+\n",
      "|DEST_COUNTRY_NAME|  destination|u_destination|\n",
      "+-----------------+-------------+-------------+\n",
      "|    United States|United States|UNITED STATES|\n",
      "|    United States|United States|UNITED STATES|\n",
      "+-----------------+-------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "flightsDF.select(\n",
    "    column(\"DEST_COUNTRY_NAME\"),\n",
    "    # flexible referencing for columns\n",
    "    expr(\"DEST_COUNTRY_NAME as destination\"),\n",
    "    # with expr() we also can apply string manipulation\n",
    "    expr(\"upper(DEST_COUNTRY_NAME) as u_destination\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **selectExpr()** method when you want to specify aggregations over the entire DataFrame, e.g. calculate the avg value of a specific columns or count the number of distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+\n",
      "|  avg_count|num_of_dest_countries|\n",
      "+-----------+---------------------+\n",
      "|1770.765625|                  132|\n",
      "+-----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.selectExpr(\"avg(count) as avg_count\", \"count(distinct(DEST_COUNTRY_NAME)) as num_of_dest_countries\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection / Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "|               Malta|      United States|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|            Suriname|      United States|    1|\n",
      "|       United States|             Cyprus|    1|\n",
      "|        Burkina Faso|      United States|    1|\n",
      "|            Djibouti|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "|               Malta|      United States|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|            Suriname|      United States|    1|\n",
      "|       United States|             Cyprus|    1|\n",
      "|        Burkina Faso|      United States|    1|\n",
      "|            Djibouti|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The following statements aree the same:\n",
    "flightsDF.filter(col(\"count\") < 2).show(10)\n",
    "flightsDF.where(\"count < 2\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting\n",
    "\n",
    "Keep in mind, sorting is a wide operation, requires data exchange among Spark worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "|    United States|             Mexico|  7187|\n",
      "|           Mexico|      United States|  7140|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "flightsDF.sort(\"count\").show(5)\n",
    "flightsDF.sort(column(\"count\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.orderBy(expr(\"count desc\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Nulls\n",
    "\n",
    "Nulls are a challenging part of all programming, and Spark is no exception. In our opinion, being\n",
    "explicit is always better than being implicit when handling null values. For instance, in this part of the\n",
    "book, we saw how we can define columns as having null types. However, this comes with a catch.\n",
    "When we declare a column as not having a null time, that is not actually enforced. To reiterate, when\n",
    "you define a schema in which all columns are declared to not have null values, Spark will not enforce\n",
    "that and will happily let null values into that column. The nullable signal is simply to help Spark SQL\n",
    "optimize for handling that column. If you have null values in columns that should not have null values,\n",
    "you can get an incorrect result or see strange exceptions that can be difficult to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRow1 = Row(\"Hello\", None)\n",
    "myRow2 = Row(None, 12)\n",
    "myDF = spark.createDataFrame([myRow1, myRow2], mySchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**coalesce():** returns the first non-null value from a column (this is a different coalesce() from what we used in the previous section for combining partitions!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------------------+\n",
      "| colA|colB|coalesce(colA, colB)|\n",
      "+-----+----+--------------------+\n",
      "|Hello|null|               Hello|\n",
      "| null|  12|                  12|\n",
      "+-----+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "myDF.select(column(\"colA\"), column(\"colB\"), coalesce(column(\"colA\"), column(\"colB\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------------------------------+\n",
      "| colA|colB|coalesce(colA, CAST(colB AS STRING))|\n",
      "+-----+----+------------------------------------+\n",
      "|Hello|null|                               Hello|\n",
      "| null|  12|                                  12|\n",
      "+-----+----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Just to show the same in SQL\n",
    "myDF.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT colA, colB, coalesce(colA, colB)\n",
    "FROM df\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other useful functions:\n",
    "\n",
    "**ifnull(val1, val2):** allows you to use val2 is val1 == null\n",
    "\n",
    "**nullif(val1, val2):** if val1 == val2 then returns null or else returns val2\n",
    "\n",
    "**nvl2(val1, val2, val3):** if val1 == null: return val2 else return val3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------+------------------------------------------------------+\n",
      "|ifnull(NULL, 'return_value')|nullif('value', 'value')|nvl2('not_a_null_value', 'return_value', 'else_value')|\n",
      "+----------------------------+------------------------+------------------------------------------------------+\n",
      "|                return_value|                    null|                                          return_value|\n",
      "+----------------------------+------------------------+------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    ifnull(null, 'return_value'),\n",
    "    nullif('value', 'value'),\n",
    "    nvl2('not_a_null_value', 'return_value', 'else_value')\n",
    "FROM df LIMIT 1\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **drop()** operation to remove any rows which contain null value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| colA|colB|\n",
      "+-----+----+\n",
      "|Hello|null|\n",
      "| null|  12|\n",
      "+-----+----+\n",
      "\n",
      "+----+----+\n",
      "|colA|colB|\n",
      "+----+----+\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF.show()\n",
    "myDF.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|colA|colB|\n",
      "+----+----+\n",
      "|null|  12|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# or define specific columns to drop\n",
    "myDF.na.drop(\"all\", subset=['ColB']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| colA|colB|\n",
      "+-----+----+\n",
      "|Hello|null|\n",
      "| null|  12|\n",
      "+-----+----+\n",
      "\n",
      "+----+----+\n",
      "|colA|colB|\n",
      "+----+----+\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all vs any\n",
    "myDF.na.drop(\"all\").show()\n",
    "myDF.na.drop(\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| colA|colB|\n",
      "+-----+----+\n",
      "|Hello|null|\n",
      "|    a|  12|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF.na.fill(\"a\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| colA|colB|\n",
      "+-----+----+\n",
      "|Hello|   0|\n",
      "|    a|  12|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fillMap = { 'colA': 'a', 'colB': 0 }\n",
    "myDF.na.fill(fillMap).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|split(DEST_COUNTRY_NAME, \\s, -1)|\n",
      "+--------------------------------+\n",
      "|                [United, States]|\n",
      "|                [United, States]|\n",
      "|                [United, States]|\n",
      "|                         [Egypt]|\n",
      "|                [United, States]|\n",
      "|                [United, States]|\n",
      "|                [United, States]|\n",
      "|                   [Costa, Rica]|\n",
      "|                       [Senegal]|\n",
      "|                       [Moldova]|\n",
      "|                [United, States]|\n",
      "|                [United, States]|\n",
      "|                        [Guyana]|\n",
      "|                         [Malta]|\n",
      "|                      [Anguilla]|\n",
      "|                       [Bolivia]|\n",
      "|                [United, States]|\n",
      "|                       [Algeria]|\n",
      "|            [Turks, and, Caic...|\n",
      "|                [United, States]|\n",
      "+--------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "flightsDF.select(split(col('DEST_COUNTRY_NAME'), '\\\\s')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
