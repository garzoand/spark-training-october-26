{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key benefits of DataFrames\n",
    "\n",
    "In this module we will deep dive a bit more into the RDD API and see what are the key differences between RDDs and Dataframes. Moreover, unlike in the previous excercise, now we're performing some aggregation on top of the the data as well.\n",
    "\n",
    "Let's start with the following simple excersice: each student took different number of tests across the year and we would like to calculate the average or scores they achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume for now, our data just a list of tuples with name and the achieved score on a particular exam\n",
    "data = [(\"Andras\", 10), (\"Bob\", 20), (\"Bob\", 30), (\"Andras\", 12), (\"Bob\", 35)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #1: Calculating avg. scores through the RDD API\n",
    "\n",
    "Since Spark does not really know anything about the data stored in the RDD, we have to write a very explicit code on how to calculate these averages:\n",
    "\n",
    "0. First you need to copy the data from a python list into the Spark framework. We can use the SparkContext's **parallelize()** method as we did in the previous lab.\n",
    "\n",
    "1. First we need to transform the dataset into key-value pairs. This should not be very difficult, as the Name will be the key. However, for the value, we also need to add an extra field which keeps track the number of elements as we'll need this value when calculating the avg.\n",
    "\n",
    "2. We need to perform a **reduceByKey** operation. This is a simple **reduce** operation, but it is performed just on top of the list of values which has the same key value. During the reduce operation we just sum up all the value field (scores and number of elements)\n",
    "\n",
    "3. The last computation step is to calculate the average itself, where we need to devide the sum of scores by the number of elements\n",
    "\n",
    "4. Use the RDD's **collect()** method to copy the result RDD back to a regular python list to be able to print out the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bob', 28.333333333333332), ('Andras', 11.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# for each key: overall sum, number of element \n",
    "\n",
    "# Andras: [(10, 1), (12, 1)]\n",
    "# reduceByKey(lambda value1, value2: (value1[0] + value2[0], value1[1] + value2[1]))\n",
    "\n",
    "rdd = rdd \\\n",
    "    .map(lambda x: (x[0], (x[1], 1))) \\\n",
    "    .reduceByKey(lambda value1, value2: (value1[0] + value2[0], value1[1] + value2[1])) \\\n",
    "    .map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "\n",
    "# (Andras, (sum, count) -> (Andras, sum / count))\n",
    "\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(data)\n",
    "\n",
    "step1 = rdd.groupByKey()\n",
    "step2 = step1.map(lambda student: (student[0], sum(student[1])/sum(1 for _ in student[1])))\n",
    "\n",
    "avg_rdd = step2\n",
    "\n",
    "avg_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What are the drawbacks of this solution?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #2: Calculating avg. scores with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the same logic above with the DataFrame API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  Name|Score|\n",
      "+------+-----+\n",
      "|Andras|   10|\n",
      "|   Bob|   20|\n",
      "|   Bob|   30|\n",
      "|Andras|   12|\n",
      "|   Bob|   35|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The data is already structured (has 2 columns), so we can create a dataframe.\n",
    "df = spark.createDataFrame(data, ['Name', 'Score'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Score: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|  Name|       final_score|\n",
      "+------+------------------+\n",
      "|Andras|              11.0|\n",
      "|   Bob|28.333333333333332|\n",
      "+------+------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[Name#13], functions=[avg(Score#14L)])\n",
      "+- Exchange hashpartitioning(Name#13, 200), true, [id=#165]\n",
      "   +- *(1) HashAggregate(keys=[Name#13], functions=[partial_avg(Score#14L)])\n",
      "      +- *(1) Scan ExistingRDD[Name#13,Score#14L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('scores')\n",
    "result_df = spark.sql(\"\"\"\n",
    "SELECT Name, avg(Score) as final_score\n",
    "FROM scores\n",
    "GROUP BY Name;\n",
    "\"\"\")\n",
    "result_df.show()\n",
    "result_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|  Name|       final_score|\n",
      "+------+------------------+\n",
      "|Andras|              11.0|\n",
      "|   Bob|28.333333333333332|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Name').agg(avg('Score').alias('final_score')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: what are the advantages of the DataFrame API over RDDs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
