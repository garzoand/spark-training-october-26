{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames with SQL\n",
    "\n",
    "\n",
    "We talked about before that Spark is a unified computing engine. Among many things, it also means that Spark is not tied to one specific language. You can have access to the same transformation supported by the engine regardless if you're using the DF API or SQL.\n",
    "\n",
    "When you express your logic in SQL, spark will compile the expression into an underlying execution plan built from primitives supported by the core engine. After that the job will be executed in the same way as we would have used the legacy spark API.\n",
    "\n",
    "Let's see an example on how we can use SQL expressions with Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a dataframe in the same way we did in the previous module. We'll use the same sample dataset to create the DF with automated schema inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF = spark \\\n",
    "    .read \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('../data/flights.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to turn the DF into a table or a view. This can be done with one simple method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a View on top of the DF which can be used for querying with SQL\n",
    "flightsDF.createOrReplaceTempView(\"flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|total_destination|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as total_destination\n",
    "FROM flights\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY total_destination DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a comparison, let's see how the same expression would like with using the DataFrames API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|total_destination|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "resDF = flightsDF \\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\") \\\n",
    "    .sum(\"count\") \\\n",
    "    .withColumnRenamed(\"sum(count)\", \"total_destination\") \\\n",
    "    .sort(desc(\"total_destination\")) \\\n",
    "    .limit(5)\n",
    "resDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the physical execution plan created by the two queries. We can see that the same transformations are triggered in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[total_destination#22L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#16,total_destination#22L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[sum(cast(count#18 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#16, 200), true, [id=#117]\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[partial_sum(cast(count#18 as bigint))])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#16,count#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/andras/ipython_spark/spark_training_baseline/data/flights.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[total_destination#47L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#16,total_destination#47L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[sum(cast(count#18 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#16, 200), true, [id=#141]\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[partial_sum(cast(count#18 as bigint))])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#16,count#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/andras/ipython_spark/spark_training_baseline/data/flights.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.explain()\n",
    "\n",
    "resDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managed SQL Tables and Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql('DROP DATABASE test_db')\n",
    "spark.sql('CREATE DATABASE test_db')\n",
    "spark.sql('USE test_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark stores table metainformation in a Hive metastore which is by default located under the spark-warehouse subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "drwxr-xr-x 1 andras andras 4096 Oct  5 11:48 test_db.db\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l spark-warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF = spark \\\n",
    "    .read \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('../data/flights.csv')\n",
    "flightsDF.write.saveAsTable('flights_managed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM flights_managed LIMIT 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('drop table flights_managed')\n",
    "spark.sql('drop database test_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're creating a Spark Managed Table as above, Spark not just managing the table metadata but the data itself as well. If you drop a managed table, the data will be also deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unmanaged tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('create database test_db')\n",
    "spark.sql(\"CREATE TABLE flights_unmanaged(DEST_COUNTRY_NAME string, ORIG_COUNTRY_NAME string, count int) using csv options (path '/tmp/flights.csv')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIG_COUNTRY_NAME|count|\n",
      "+-----------------+-----------------+-----+\n",
      "+-----------------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from flights_unmanaged limit 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF \\\n",
    "    .write \\\n",
    "    .option('path', '/tmp/flights.csv') \\\n",
    "    .insertInto('flights_unmanaged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIG_COUNTRY_NAME|count|\n",
      "+-----------------+-----------------+-----+\n",
      "|    United States|          Romania|   15|\n",
      "|    United States|          Croatia|    1|\n",
      "|    United States|          Ireland|  344|\n",
      "|            Egypt|    United States|   15|\n",
      "|    United States|            India|   62|\n",
      "|    United States|        Singapore|    1|\n",
      "|    United States|          Grenada|   62|\n",
      "|       Costa Rica|    United States|  588|\n",
      "|          Senegal|    United States|   40|\n",
      "|          Moldova|    United States|    1|\n",
      "+-----------------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from flights_unmanaged limit 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
