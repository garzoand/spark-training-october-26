{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations with DataFrames\n",
    "\n",
    "Aggregations are all wide tranformations, i.e. requires shuffling during the execution. Moreover, transformations involving data shuffling are synchronous operations, that means we cannot move forward to the next operation in the pipeline until all the workers have not completed with the shuffling transformation. Contrary to narrow operations, which are fully asynchronous as there is no data exchange, hence no coordination needed among worker nodes to execute the pipeline.\n",
    "\n",
    "Here are the list of the aggregation operations available for Dataframes:\n",
    "\n",
    "* count(), sum() and other expressions use in the SELECT statement\n",
    "\n",
    "* groupBy()\n",
    "\n",
    "* window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-153f8b0be037>:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-153f8b0be037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark3_jupyter/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/spark3_jupyter/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    334\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 336\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-153f8b0be037>:4 "
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('../data/retail.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "StructType(List(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,StringType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,IntegerType,true),StructField(Country,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "print (df.rdd.getNumPartitions())\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"retail\")\n",
    "print (df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count and Count Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n",
      "[Row(count(DISTINCT StockCode)=4070)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, countDistinct\n",
    "df.select(count('StockCode')).show()\n",
    "res = df.select(countDistinct('StockCode')).collect()\n",
    "print (res)\n",
    "exact_count = res[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real-world Big Data application often we're not really interested in the preciese result, a reasonable estimation is good enough. That's why using probabilistic approximations are often can help us out when we take care of the performance and execution time but having a precise answer is less important. Spark offers some functions which can estimate the distinct count for example, but runs much faster than its exact counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(approx_count_distinct(StockCode)=4079)]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "max_estimated_error = 0.01\n",
    "res = df.select(approx_count_distinct('StockCode', max_estimated_error)).collect()\n",
    "print(res)\n",
    "est_count = res[0][0]\n",
    "print (abs(exact_count - est_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "|   538800|     16458|   10|\n",
      "|   538942|     17346|   12|\n",
      "|  C539947|     13854|    1|\n",
      "|   540096|     13253|   16|\n",
      "|   540530|     14755|   27|\n",
      "|   541225|     14099|   19|\n",
      "|   541978|     13551|    4|\n",
      "|   542093|     17677|   16|\n",
      "|   543188|     12567|   63|\n",
      "|   543590|     17377|   19|\n",
      "|  C543757|     13115|    1|\n",
      "|  C544318|     12989|    1|\n",
      "|   544578|     12365|    1|\n",
      "|   536596|      null|    6|\n",
      "|   537252|      null|    1|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|InvoiceNo|count(Quantity)|\n",
      "+---------+---------------+\n",
      "|   536596|              6|\n",
      "|   536938|             14|\n",
      "|   537252|              1|\n",
      "|   537691|             20|\n",
      "|   538041|              1|\n",
      "|   538184|             26|\n",
      "|   538517|             53|\n",
      "|   538879|             19|\n",
      "|   539275|              6|\n",
      "|   539630|             12|\n",
      "|   540499|             24|\n",
      "|   540540|             22|\n",
      "|  C540850|              1|\n",
      "|   540976|             48|\n",
      "|   541432|              4|\n",
      "|   541518|            101|\n",
      "|   541783|             35|\n",
      "|   542026|              9|\n",
      "|   542375|              6|\n",
      "|  C542604|              8|\n",
      "+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df.groupBy(\"InvoiceNo\") \\\n",
    "    .agg(expr(\"count(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|InvoiceNo|count(Quantity)|\n",
      "+---------+---------------+\n",
      "|   536596|              6|\n",
      "|   536938|             14|\n",
      "|   537252|              1|\n",
      "|   537691|             20|\n",
      "|   538041|              1|\n",
      "|   538184|             26|\n",
      "|   538517|             53|\n",
      "|   538879|             19|\n",
      "|   539275|              6|\n",
      "|   539630|             12|\n",
      "|   540499|             24|\n",
      "|   540540|             22|\n",
      "|  C540850|              1|\n",
      "|   540976|             48|\n",
      "|   541432|              4|\n",
      "|   541518|            101|\n",
      "|   541783|             35|\n",
      "|   542026|              9|\n",
      "|   542375|              6|\n",
      "|  C542604|              8|\n",
      "+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\") \\\n",
    "    .agg(count(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window based Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,StringType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,IntegerType,true),StructField(Country,StringType,true),StructField(date,DateType,true)))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "# adds date column which converts invoice date string to a date type\n",
    "dfWithDate = df.withColumn('date', to_date(\"InvoiceDate\", 'MM/d/yyyy H:mm'))\n",
    "dfWithDate.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, column, max, rank, dense_rank\n",
    "windowSpec = Window \\\n",
    "    .partitionBy(\"CustomerId\", \"date\") \\\n",
    "    .orderBy(desc(\"Quantity\")) \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxPurchaseQuantity = max(column(\"Quantity\")).over(windowSpec)\n",
    "type(maxPurchaseQuantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+----+-----+-----+\n",
      "|CustomerID|      date|Quantity|MaxQ|Prank|Prank|\n",
      "+----------+----------+--------+----+-----+-----+\n",
      "|     12477|2011-04-14|     100| 100|    1|    1|\n",
      "|     12477|2011-04-14|      72| 100|    2|    2|\n",
      "|     12477|2011-04-14|      36| 100|    3|    3|\n",
      "|     12477|2011-04-14|      36| 100|    3|    3|\n",
      "|     12477|2011-04-14|      36| 100|    3|    3|\n",
      "|     12477|2011-04-14|      24| 100|    6|    4|\n",
      "|     12477|2011-04-14|      24| 100|    6|    4|\n",
      "|     12477|2011-04-14|      24| 100|    6|    4|\n",
      "|     12477|2011-04-14|      20| 100|    9|    5|\n",
      "|     12477|2011-04-14|      12| 100|   10|    6|\n",
      "|     12477|2011-04-14|      12| 100|   10|    6|\n",
      "|     12477|2011-04-14|      12| 100|   10|    6|\n",
      "|     12477|2011-04-14|      12| 100|   10|    6|\n",
      "|     12477|2011-04-14|      12| 100|   10|    6|\n",
      "|     12477|2011-04-14|      12| 100|   10|    6|\n",
      "|     12477|2011-04-14|      12| 100|   10|    6|\n",
      "|     12477|2011-04-14|      12| 100|   10|    6|\n",
      "|     12477|2011-04-14|      12| 100|   10|    6|\n",
      "|     12477|2011-04-14|      12| 100|   10|    6|\n",
      "|     12477|2011-04-14|      12| 100|   10|    6|\n",
      "|     12477|2011-04-14|      12| 100|   10|    6|\n",
      "|     12477|2011-04-14|      12| 100|   10|    6|\n",
      "|     12477|2011-04-14|      12| 100|   10|    6|\n",
      "|     12477|2011-04-14|      12| 100|   10|    6|\n",
      "|     12477|2011-04-14|      12| 100|   10|    6|\n",
      "|     12477|2011-04-14|      10| 100|   26|    7|\n",
      "|     12477|2011-04-14|      10| 100|   26|    7|\n",
      "|     12477|2011-04-14|      10| 100|   26|    7|\n",
      "|     12477|2011-04-14|       8| 100|   29|    8|\n",
      "|     12477|2011-04-14|       8| 100|   29|    8|\n",
      "|     12477|2011-04-14|       6| 100|   31|    9|\n",
      "|     12477|2011-04-14|       6| 100|   31|    9|\n",
      "|     12477|2011-04-14|       6| 100|   31|    9|\n",
      "|     12477|2011-04-14|       6| 100|   31|    9|\n",
      "|     12477|2011-04-14|       6| 100|   31|    9|\n",
      "|     12477|2011-04-14|       6| 100|   31|    9|\n",
      "|     12477|2011-04-14|       6| 100|   31|    9|\n",
      "|     12477|2011-04-14|       6| 100|   31|    9|\n",
      "|     12477|2011-04-14|       6| 100|   31|    9|\n",
      "|     12477|2011-04-14|       6| 100|   31|    9|\n",
      "|     12477|2011-04-14|       6| 100|   31|    9|\n",
      "|     12477|2011-04-14|       6| 100|   31|    9|\n",
      "|     12477|2011-04-14|       5| 100|   43|   10|\n",
      "|     12477|2011-04-14|       5| 100|   43|   10|\n",
      "|     12477|2011-04-14|       5| 100|   43|   10|\n",
      "|     12477|2011-04-14|       4| 100|   46|   11|\n",
      "|     12477|2011-04-14|       4| 100|   46|   11|\n",
      "|     12477|2011-04-14|       3| 100|   48|   12|\n",
      "|     12477|2011-04-14|       3| 100|   48|   12|\n",
      "|     12477|2011-04-14|       3| 100|   48|   12|\n",
      "|     12477|2011-04-14|       2| 100|   51|   13|\n",
      "|     12477|2011-04-14|       2| 100|   51|   13|\n",
      "|     12477|2011-04-14|       2| 100|   51|   13|\n",
      "|     12477|2011-04-14|       2| 100|   51|   13|\n",
      "|     12477|2011-04-14|       2| 100|   51|   13|\n",
      "|     12477|2011-04-14|       2| 100|   51|   13|\n",
      "|     12477|2011-04-14|       2| 100|   51|   13|\n",
      "|     12569|2011-04-14|      24|  24|    1|    1|\n",
      "|     12569|2011-04-14|      16|  24|    2|    2|\n",
      "|     12569|2011-04-14|      12|  24|    3|    3|\n",
      "|     12569|2011-04-14|      12|  24|    3|    3|\n",
      "|     12569|2011-04-14|      12|  24|    3|    3|\n",
      "|     12569|2011-04-14|      12|  24|    3|    3|\n",
      "|     12569|2011-04-14|      12|  24|    3|    3|\n",
      "|     12569|2011-04-14|      12|  24|    3|    3|\n",
      "|     12569|2011-04-14|      12|  24|    3|    3|\n",
      "|     12569|2011-04-14|      12|  24|    3|    3|\n",
      "|     12569|2011-04-14|      12|  24|    3|    3|\n",
      "|     12569|2011-04-14|       6|  24|   12|    4|\n",
      "|     12569|2011-04-14|       6|  24|   12|    4|\n",
      "|     12569|2011-04-14|       6|  24|   12|    4|\n",
      "|     12569|2011-04-14|       6|  24|   12|    4|\n",
      "|     12569|2011-04-14|       4|  24|   16|    5|\n",
      "|     12569|2011-04-14|       4|  24|   16|    5|\n",
      "|     12569|2011-04-14|       2|  24|   18|    6|\n",
      "|     12637|2011-04-14|      36|  36|    1|    1|\n",
      "|     12637|2011-04-14|      24|  36|    2|    2|\n",
      "|     12637|2011-04-14|      12|  36|    3|    3|\n",
      "|     12637|2011-04-14|      12|  36|    3|    3|\n",
      "|     12637|2011-04-14|      12|  36|    3|    3|\n",
      "|     12637|2011-04-14|      12|  36|    3|    3|\n",
      "|     12637|2011-04-14|      12|  36|    3|    3|\n",
      "|     12637|2011-04-14|      12|  36|    3|    3|\n",
      "|     12637|2011-04-14|      12|  36|    3|    3|\n",
      "|     12637|2011-04-14|      12|  36|    3|    3|\n",
      "|     12637|2011-04-14|      12|  36|    3|    3|\n",
      "|     12637|2011-04-14|      12|  36|    3|    3|\n",
      "|     12637|2011-04-14|      10|  36|   13|    4|\n",
      "|     12637|2011-04-14|      10|  36|   13|    4|\n",
      "|     12637|2011-04-14|      10|  36|   13|    4|\n",
      "|     12637|2011-04-14|      10|  36|   13|    4|\n",
      "|     12637|2011-04-14|      10|  36|   13|    4|\n",
      "|     12637|2011-04-14|       8|  36|   18|    5|\n",
      "|     12637|2011-04-14|       8|  36|   18|    5|\n",
      "|     12637|2011-04-14|       6|  36|   20|    6|\n",
      "|     12637|2011-04-14|       6|  36|   20|    6|\n",
      "|     12637|2011-04-14|       4|  36|   22|    7|\n",
      "|     12637|2011-04-14|       4|  36|   22|    7|\n",
      "|     12637|2011-04-14|       4|  36|   22|    7|\n",
      "|     12637|2011-04-14|       4|  36|   22|    7|\n",
      "+----------+----------+--------+----+-----+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "dfWithDate.select(\n",
    "    column(\"CustomerID\"),\n",
    "    column(\"date\"),\n",
    "    column(\"Quantity\"),\n",
    "    maxPurchaseQuantity.alias(\"MaxQ\")\n",
    ").show(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
