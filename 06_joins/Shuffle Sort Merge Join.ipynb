{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = spark.read.format('csv').option('header', 'true').load('users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.format('csv').option('header', 'true').load('orders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+-------+---+---------+----------+\n",
      "|order_id|qty|product_id|user_id|uid|user_name|user_state|\n",
      "+--------+---+----------+-------+---+---------+----------+\n",
      "|       0| 44|         4|     62| 62|  user_62|        MI|\n",
      "|       1| 61|         0|      5|  5|   user_5|        NY|\n",
      "|       2| 40|         4|     13| 13|  user_13|        CO|\n",
      "|       3| 98|         2|     31| 31|  user_31|        CA|\n",
      "|       4| 98|         3|      9|  9|   user_9|        MI|\n",
      "|       5| 91|         4|     28| 28|  user_28|        CA|\n",
      "|       6| 18|         4|     28| 28|  user_28|        CA|\n",
      "|       7| 69|         3|     69| 69|  user_69|        WA|\n",
      "|       8| 66|         4|     78| 78|  user_78|        CO|\n",
      "|       9| 57|         1|      7|  7|   user_7|        WA|\n",
      "|      10| 79|         4|     32| 32|  user_32|        CO|\n",
      "|      11| 46|         2|     22| 22|  user_22|        AZ|\n",
      "|      12| 38|         2|     38| 38|  user_38|        NY|\n",
      "|      13| 94|         1|     83| 83|  user_83|        NY|\n",
      "|      14| 73|         2|     73| 73|  user_73|        MI|\n",
      "|      15| 96|         1|     65| 65|  user_65|        MI|\n",
      "|      16| 15|         4|     36| 36|  user_36|        NY|\n",
      "|      17| 12|         2|     58| 58|  user_58|        MI|\n",
      "|      18| 85|         1|      7|  7|   user_7|        WA|\n",
      "|      19| 21|         3|      3|  3|   user_3|        CO|\n",
      "+--------+---+----------+-------+---+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedDF = orders.join(users, users.uid == orders.user_id)\n",
    "joinedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [user_id#41], [uid#16], Inner, BuildRight\n",
      ":- *(2) Project [order_id#38, qty#39, product_id#40, user_id#41]\n",
      ":  +- *(2) Filter isnotnull(user_id#41)\n",
      ":     +- FileScan csv [order_id#38,qty#39,product_id#40,user_id#41] Batched: false, DataFilters: [isnotnull(user_id#41)], Format: CSV, Location: InMemoryFileIndex[file:/home/andras/ipython_spark/spark_training_baseline/06_joins/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<order_id:string,qty:string,product_id:string,user_id:string>\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])), [id=#103]\n",
      "   +- *(1) Project [uid#16, user_name#17, user_state#18]\n",
      "      +- *(1) Filter isnotnull(uid#16)\n",
      "         +- FileScan csv [uid#16,user_name#17,user_state#18] Batched: false, DataFilters: [isnotnull(uid#16)], Format: CSV, Location: InMemoryFileIndex[file:/home/andras/ipython_spark/spark_training_baseline/06_joins/users.csv], PartitionFilters: [], PushedFilters: [IsNotNull(uid)], ReadSchema: struct<uid:string,user_name:string,user_state:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = 100000\n",
    "\n",
    "states = ['WA', 'CA', 'NY', 'MI', 'AZ', 'CO']\n",
    "products = ['ITEM1', 'ITEM2', 'ITEM3', 'ITEM4', 'ITEM5']\n",
    "\n",
    "import random\n",
    "users = sc.parallelize(range(total_users)) \\\n",
    "    .map(lambda id: (id, 'user_' + str(id), states[random.randrange(len(states))])) \\\n",
    "    .toDF(['uid', 'user_name', 'user_state'])\n",
    "\n",
    "users.write.format('csv').mode('overwrite').option('header', 'true').save('user_big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+-------+---+---------+----------+\n",
      "|order_id|qty|product_id|user_id|uid|user_name|user_state|\n",
      "+--------+---+----------+-------+---+---------+----------+\n",
      "|      90| 55|         1|     26| 26|  user_26|        CA|\n",
      "|     107| 65|         2|     26| 26|  user_26|        CA|\n",
      "|     498| 74|         0|     26| 26|  user_26|        CA|\n",
      "|     536| 38|         4|     26| 26|  user_26|        CA|\n",
      "|    1015| 88|         4|     26| 26|  user_26|        CA|\n",
      "|    1060| 61|         2|     26| 26|  user_26|        CA|\n",
      "|    1300| 82|         1|     26| 26|  user_26|        CA|\n",
      "|    1395| 92|         2|     26| 26|  user_26|        CA|\n",
      "|    1789| 54|         2|     26| 26|  user_26|        CA|\n",
      "|    1919| 67|         3|     26| 26|  user_26|        CA|\n",
      "|    2127| 41|         2|     26| 26|  user_26|        CA|\n",
      "|    2251| 21|         4|     26| 26|  user_26|        CA|\n",
      "|    2476| 74|         1|     26| 26|  user_26|        CA|\n",
      "|    2491|  3|         3|     26| 26|  user_26|        CA|\n",
      "|    2534| 38|         0|     26| 26|  user_26|        CA|\n",
      "|    2544| 80|         3|     26| 26|  user_26|        CA|\n",
      "|    2643| 32|         1|     26| 26|  user_26|        CA|\n",
      "|    2834| 60|         0|     26| 26|  user_26|        CA|\n",
      "|    2882|  7|         4|     26| 26|  user_26|        CA|\n",
      "|    3010| 56|         2|     26| 26|  user_26|        CA|\n",
      "+--------+---+----------+-------+---+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedDF = orders.join(users, users.uid == orders.user_id)\n",
    "joinedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [cast(user_id#41 as bigint)], [uid#190L], Inner\n",
      ":- *(2) Sort [cast(user_id#41 as bigint) ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(cast(user_id#41 as bigint), 200), true, [id=#300]\n",
      ":     +- *(1) Project [order_id#38, qty#39, product_id#40, user_id#41]\n",
      ":        +- *(1) Filter isnotnull(user_id#41)\n",
      ":           +- FileScan csv [order_id#38,qty#39,product_id#40,user_id#41] Batched: false, DataFilters: [isnotnull(user_id#41)], Format: CSV, Location: InMemoryFileIndex[file:/home/andras/ipython_spark/spark_training_baseline/06_joins/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<order_id:string,qty:string,product_id:string,user_id:string>\n",
      "+- *(4) Sort [uid#190L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(uid#190L, 200), true, [id=#306]\n",
      "      +- *(3) Filter isnotnull(uid#190L)\n",
      "         +- *(3) Scan ExistingRDD[uid#190L,user_name#191,user_state#192]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to speed up?\n",
    "from pyspark.sql.functions import asc\n",
    "\n",
    "users.orderBy(asc('uid')) \\\n",
    "  .write.format('csv') \\\n",
    "  .bucketBy(6, 'uid') \\\n",
    "  .mode('overwrite') \\\n",
    "  .saveAsTable('user_sorted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.orderBy(asc('user_id')) \\\n",
    "  .write.format('csv') \\\n",
    "  .bucketBy(6, 'user_id') \\\n",
    "  .mode('overwrite') \\\n",
    "  .saveAsTable('orders_sorted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('cache table user_sorted')\n",
    "spark.sql('cache table orders_sorted')\n",
    "users_sorted = spark.table('user_sorted')\n",
    "orders_sorted = spark.table('orders_sorted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [cast(user_id#930 as bigint)], [uid#788L], Inner, BuildRight\n",
      ":- *(2) Filter isnotnull(user_id#930)\n",
      ":  +- Scan In-memory table `default`.`orders_sorted` [order_id#927, qty#928, product_id#929, user_id#930], [isnotnull(user_id#930)]\n",
      ":        +- InMemoryRelation [order_id#927, qty#928, product_id#929, user_id#930], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      ":              +- FileScan csv default.orders_sorted[order_id#927,qty#928,product_id#929,user_id#930] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/andras/ipython_spark/spark_training_baseline/06_joins/spark-warehous..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,qty:string,product_id:string,user_id:string>, SelectedBucketsCount: 6 out of 6\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false])), [id=#690]\n",
      "   +- *(1) Filter isnotnull(uid#788L)\n",
      "      +- Scan In-memory table `default`.`user_sorted` [uid#788L, user_name#789, user_state#790], [isnotnull(uid#788L)]\n",
      "            +- InMemoryRelation [uid#788L, user_name#789, user_state#790], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                  +- FileScan csv default.user_sorted[uid#788L,user_name#789,user_state#790] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/andras/ipython_spark/spark_training_baseline/06_joins/spark-warehous..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<uid:bigint,user_name:string,user_state:string>, SelectedBucketsCount: 6 out of 6\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedDF = orders_sorted.join(users_sorted, users_sorted.uid == orders_sorted.user_id)\n",
    "joinedDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
