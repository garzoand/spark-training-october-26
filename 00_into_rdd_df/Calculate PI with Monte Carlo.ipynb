{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1: Calculate PI in parallel with Apache Spark using the Monte Carlo Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a circle of radius 0.5, enclosed by a 1 × 1 square. The area of the circle is πr2=π/4, the area of the square is 1. If we divide the area of the circle, by the area of the square we get π/4.\n",
    "\n",
    "\n",
    "Total Number of points: 0\n",
    "Points within circle: 0\n",
    "Pi estimation:\n",
    "Add points one-by-one\n",
    "Animate\n",
    "Speed\n",
    "Reset\n",
    " Open with CodePen\n",
    "One method to estimate the value of π (3.141592...) is by using a Monte Carlo method. In the demo above, we have a circle of radius 0.5, enclosed by a 1 × 1 square. The area of the circle is πr2=π/4, the area of the square is 1. If we divide the area of the circle, by the area of the square we get π/4.\n",
    "\n",
    "We then generate a large number of uniformly distributed random points and plot them on the graph. These points can be in any position within the square i.e. between (0,0) and (1,1). If they fall within the circle, they are coloured red, otherwise they are coloured blue. We keep track of the total number of points, and the number of points that are inside the circle. If we divide the number of points within the circle, Ninner by the total number of points, Ntotal, we should get a value that is an approximation of the ratio of the areas we calculated above, π/4.\n",
    "\n",
    "In other words,\n",
    "\n",
    "π4 ≈ Ninner / Ntotal\n",
    "\n",
    "π ≈ 4Ninner / Ntotal\n",
    "\n",
    "<img src='./montecarlo.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark context\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of points to be used during the simulation\n",
    "num_samples = 100000\n",
    "# num_samples = 100000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_point(p):\n",
    "    # just generate two random x and y coordinates between (0, 1), regardless the input\n",
    "    x = random.random()\n",
    "    y = random.random()\n",
    "    return (x, y)\n",
    "\n",
    "# point is a tuple (x, y)\n",
    "def inside_the_circle(point):\n",
    "    # returns true if the point is in the circle\n",
    "    x = point[0]\n",
    "    y = point[1]\n",
    "    return (x**2 + y**2 < 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.14172"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = range(100000)\n",
    "k = len(list(filter(inside_the_circle, map(gen_random_point, l))))\n",
    "4 * (k / 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7733220271447795, 0.5451749199388612),\n",
       " (0.16667746438446662, 0.3861032287810393),\n",
       " (0.17811503014270713, 0.02760527504565391),\n",
       " (0.5323115568620385, 0.8082912235606978),\n",
       " (0.03581442393918355, 0.38632518110024927)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkJob = sc.parallelize(range(0, num_samples)) \\\n",
    "    .map(gen_random_point) \\\n",
    "    .filter(inside_the_circle)\n",
    "\n",
    "sparkJob.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1478\n"
     ]
    }
   ],
   "source": [
    "points_in_circle = sparkJob.count()\n",
    "\n",
    "pi_estimate = 4.0 * points_in_circle / num_samples\n",
    "\n",
    "print(pi_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the list of points into a Dataframe\n",
    "\n",
    "Dataframes like spreadsheet but stored partitioned across the servers in your spark cluster. You can do standard operations, like filtering, projections, joins, etc. what you usually can do on top of structured data, but it will be executed in parallel across the nodes. We will see dataframes in detail later during the course\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- point_x: double (nullable = true)\n",
      " |-- point_y: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "points_in_circle_df = sparkJob.toDF([\"point_x\", \"point_y\"])\n",
    "points_in_circle_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes has some nice properties. For example, you can use expressions to filter the rows and you don't need to usec closures like in the previous examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(point_x=0.9929514713106562, point_y=0.05981262003230803),\n",
       " Row(point_x=0.9454315286737294, point_y=0.012655744128695856),\n",
       " Row(point_x=0.9178649173156344, point_y=0.03058842636433068),\n",
       " Row(point_x=0.916504603690335, point_y=0.02483951575131127),\n",
       " Row(point_x=0.9455943222429517, point_y=0.07967301581441122),\n",
       " Row(point_x=0.9918079069699787, point_y=0.056238790433995045),\n",
       " Row(point_x=0.9252100032746915, point_y=0.0895698754674834),\n",
       " Row(point_x=0.9679890981573258, point_y=0.037182969426968304),\n",
       " Row(point_x=0.9728967141483011, point_y=0.046676922974952406),\n",
       " Row(point_x=0.9961561607993109, point_y=0.028224569531642207)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_in_circle_df.where(\"point_x > 0.9 and point_y < 0.1\").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you're working with RDDs or Dataframes, the result data can be copied back to the local memory of the notebook (which is your spark driver application is this case BTW!). After that you can access to this data as a regular list. To obtain the entire result dataset, you can use the **collect()** method. \n",
    "\n",
    "However, note that we're not really using the **collect()** method very often in practice. As we will see later, actually it is quite rare that we need to copy the result dataset back to the driver's local memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[Row(point_x=0.9929514713106562, point_y=0.05981262003230803), Row(point_x=0.9454315286737294, point_y=0.012655744128695856), Row(point_x=0.9178649173156344, point_y=0.03058842636433068), Row(point_x=0.916504603690335, point_y=0.02483951575131127), Row(point_x=0.9455943222429517, point_y=0.07967301581441122), Row(point_x=0.9918079069699787, point_y=0.056238790433995045), Row(point_x=0.9252100032746915, point_y=0.0895698754674834), Row(point_x=0.9679890981573258, point_y=0.037182969426968304), Row(point_x=0.9728967141483011, point_y=0.046676922974952406), Row(point_x=0.9961561607993109, point_y=0.028224569531642207)]\n"
     ]
    }
   ],
   "source": [
    "my_points = points_in_circle_df.where(\"point_x > 0.9 and point_y < 0.1\").collect()\n",
    "# see how my_points is not a spark \"data type\" anymore. It is a regular list copied to the main memory of the \n",
    "# driver application (i. e. to this notebook)\n",
    "print (type(my_points))\n",
    "# print the first 10 elements of the list\n",
    "print (my_points[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
